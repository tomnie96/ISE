{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 - ISE2020 - NLP Techniques.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomnie96/ISE/blob/master/02_ISE2020_NLP_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne-8-JKrtBht",
        "colab_type": "text"
      },
      "source": [
        "**To adapt this notebook to your own needs** and to be able to edit it, please make a copy of your own. This works via \"*File*\" -> \"*Save a copy ..*.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxpoGC4Kc3zf",
        "colab_type": "text"
      },
      "source": [
        "Some of the **NLP Techniques** mentioned [in Sect. 2.4 of the ISE 2020 lecture](https://www.slideshare.net/lysander07/02-ise2020-natural-language-processing-1-232058444) are already implemented in the [python NLTK library.](https://www.nltk.org/) Please find some basic NLP examples below.\n",
        "\n",
        "# Tokenization\n",
        "Tokenization is the process of separating character sequences into\n",
        "smaller pieces, called tokens. In this process, certain characters\n",
        "might be omitted, such as punctuation (dependening on the\n",
        "tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwS0VhZxdFjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First we have to import nltk and download a few required packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZo4osPkdiX1",
        "colab_type": "text"
      },
      "source": [
        "First let's try **Sentence Splitting**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKAo1y_FdJ-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"Mary had a little lamb. Her fleece was white as snow.\"\n",
        "# We import the two methods required for (1) word-based tokenization, and (2) sentence splitting\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "sents=sent_tokenize(text)\n",
        "print(sents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq4Z8Pw2drEr",
        "colab_type": "text"
      },
      "source": [
        "Now, let's try **Words**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VJ4dsf1dusB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=[word_tokenize(sent) for sent in sents]\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BCCTcfJebvq",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-Speech tagging\n",
        "Part-of-speech tagging classifies words into their part-of-speech\n",
        "and labels them according to a specified tagset. Most commonly\n",
        "the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4o-ztO3eQ1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# each word in the text will be assigned a POS tag\n",
        "nltk.pos_tag(word_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gprSb4QQew3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in case you don't know the meaning of some of the POS tags\n",
        "nltk.help. upenn_tagset ('NNP')\n",
        "nltk.help. upenn_tagset ('VBD')\n",
        "nltk.help. upenn_tagset ('PRP$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Cb-LBifgbM",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization\n",
        "* Lemmatization groups words together that have different inflections so that they can be treated as the same item.\n",
        "* It reduces a word to its baseform using a online lexicon. \n",
        "\n",
        "*For Lemmatization, NLTK provides an interface to the [WordNet](https://wordnet.princeton.edu/) dictionary. WordNet is a large English lexical database. Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea-fgIiJf1T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we import the WordNet lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# new text example\n",
        "sentence = \"Mary had two little lambs. Her fleeces were white as snow.\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# for each word of the sentence\n",
        "for token in word_tokenize(sentence):\n",
        "  print(lemmatizer.lemmatize(token, pos='v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KXDZz8uiim0",
        "colab_type": "text"
      },
      "source": [
        "# Stemming\n",
        "* Stemming strips the words of its suffixes and prefixes. For English, the [Porter Stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html) is rather popular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKRYxpuViNbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we import the Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "sentence = \"Mary had two little lambs. Her fleeces were white as snow.\"\n",
        "\n",
        "ps = PorterStemmer()\n",
        "# for each word of the sentence\n",
        "for token in word_tokenize(sentence):\n",
        "  print(ps.stem(token))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl-Dk-KslBRk",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition (NER)\n",
        "Locating and classifying atomic elements into predefined categories such as **names, persons, organizations, locations, expressions of time, quantities, monetary values**, etc.\n",
        "\n",
        "*For casual use, NLTK provides us with a method called `ne_chunk` to perform NER on a given text. In order to use `ne_chunk`, the text needs to first be tokenized into words and then POS tagged. After NER, the tagged words depict their respective entity type*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQB_knB_lHqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For NER, we need tokenization, POS tagging and Named Entity Chunking\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "# New text example\n",
        "sentence = \"The Avengers began as a group of extraordinary individuals who were assembled to defeat \\\n",
        "Loki and his chitauri army in New York City. \"\n",
        "print (ne_chunk(pos_tag(word_tokenize(sentence))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAZGXYlBXxbk",
        "colab_type": "text"
      },
      "source": [
        "Now let's try an **alternativ NLP Library: [spacy]**(https://spacy.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRJDsTKCVrJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBNwNzLAWGFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAjtTWnxYg1E",
        "colab_type": "text"
      },
      "source": [
        "We first start with **Named Entity Recognition **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLqgaHyFYa7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(u'Neil Alden Armstrong was an American astronaut and aeronautical engineer who was the first person to walk on the Moon.')\n",
        "\n",
        " \n",
        "# Named Entity Recognition\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "    \n",
        "# displaCy\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508M9_kMYmkR",
        "colab_type": "text"
      },
      "source": [
        "# Dependency Parsing\n",
        "Dependency Parsing is an approximation of semantic relations between arguments. It relies on direct binary grammatical relations among words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Q5mDkMXoPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependency Parsing\n",
        "\n",
        "doc = nlp(u'Neil Alden Armstrong was an American astronaut and aeronautical engineer who was the first person to walk on the Moon.')\n",
        " \n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RcDA89AY6Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing Dependency Parsing\n",
        "\n",
        "from spacy import displacy\n",
        " \n",
        "doc = nlp(u'Neil Alden Armstrong was an American astronaut and aeronautical engineer who was the first person to walk on the Moon.')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5DzJK531E0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}